{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook, we will classify materials as metals or nonmetals. The dataset that we will use is built in the `dataset_preparation.ipynb` file. We will test many possible algorithms and to assess which one gives the better accuracy. The workflow is essentially the same for all algorithms: we perform a train test split; then perform a grid search evaluated against a 5-fold split of the training set as our validation set to find the best set of hyperparameters; finally, we evaluate the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing\n",
    "import xgboost as xgb #For parallel gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset loading\n",
    "df = pd.read_csv('gap_prediction.csv')\n",
    "\n",
    "#Turning space group into a categorical variable\n",
    "df[\"Space Group\"] = df[\"Space Group\"].astype('category')\n",
    "\n",
    "#Building a dict that maps the space groups in unique integers\n",
    "mapping_dict = dict(zip(df['Space Group'], df['Space Group'].cat.codes))\n",
    "\n",
    "#Transforms the categorical space group to numbers\n",
    "df['Space Group'] = df['Space Group'].map(mapping_dict)\n",
    "\n",
    "#Target.; 1 if metal (gap==0); 0 otherwise\n",
    "y = [1 if gap==0 else 0 for gap in df['gap']]\n",
    "df.drop(['gap','Material','Unnamed: 0'], axis='columns', inplace=True)\n",
    "X = df.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcsgil/Desktop/MLPhysics/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "25 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marcsgil/Desktop/MLPhysics/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/marcsgil/Desktop/MLPhysics/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/marcsgil/Desktop/MLPhysics/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1179, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/marcsgil/Desktop/MLPhysics/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.6048041  0.66795841        nan 0.6241772  0.69120223\n",
      "        nan 0.70476362 0.7198739         nan 0.71522424 0.71677762\n",
      "        nan 0.71019072 0.71019147]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01,0.1,1,10],#, 0.1, 1, 10],  # Inverse of regularization strength\n",
    "    'penalty': ['elasticnet', 'l1', 'l2']  # Regularization penalty (L1 or L2)\n",
    "}\n",
    "\n",
    "# Create a Logistc Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=50000,solver='saga')\n",
    "\n",
    "# Use GridSearchCV to find the best combination of hyperparameters\n",
    "grid_search = GridSearchCV(lr_classifier, param_grid, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "grid_search.fit(scaler.transform(X_train), y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7167182662538699\n",
      "Cross-Validation Scores: [0.72340426 0.70736434 0.71317829 0.6996124  0.71899225]\n",
      "Mean CV Accuracy: 0.7125103084281709\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistc Regression classifier with the best hyperparameters\n",
    "best_lr_classifier = LogisticRegression(**best_params, max_iter=10000)\n",
    "best_lr_classifier.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_lr_classifier.predict(scaler.transform(X_test))\n",
    "accuracy_lr = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_lr)\n",
    "\n",
    "# Perform Cross-Validation with the best hyperparameters\n",
    "cv_scores_lr = cross_val_score(best_lr_classifier, X_train, y_train, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_lr)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],              # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],    # Kernel type (linear or radial basis function)\n",
    "    'gamma': ['scale', 'auto', 0.1]  # Kernel coefficient for 'rbf' kernel\n",
    "}\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Use GridSearchCV to find the best combination of hyperparameters\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "grid_search.fit(scaler.transform(X_train), y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7724458204334366\n",
      "Cross-Validation Scores: [0.60348162 0.60658915 0.6124031  0.62403101 0.60852713]\n",
      "Mean CV Accuracy: 0.6110064024710239\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM classifier classifier with the best hyperparameters\n",
    "best_svm_classifier = SVC(**best_params)\n",
    "best_svm_classifier.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_svm_classifier.predict(scaler.transform(X_test))\n",
    "accuracy_svm = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_svm)\n",
    "\n",
    "# Perform Cross-Validation with the best hyperparameters\n",
    "cv_scores_svm = cross_val_score(best_svm_classifier, X_train, y_train, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_svm)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Use GridSearchCV to find the best combination of hyperparameters\n",
    "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8390092879256966\n",
      "Cross-Validation Scores: [0.82978723 0.80426357 0.79457364 0.78875969 0.8003876 ]\n",
      "Mean CV Accuracy: 0.8035543460333168\n"
     ]
    }
   ],
   "source": [
    "#= Train the Decision Tree classifier with the best hyperparameters\n",
    "best_dt_classifier = DecisionTreeClassifier(**best_params)\n",
    "best_dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_dt)\n",
    "\n",
    "# Perform Cross-Validation with the best hyperparameters\n",
    "cv_scores_dt = cross_val_score(best_dt_classifier, X_train, y_train, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_dt)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Parameter Tuning with Cross-Validation\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],     # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],    # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]       # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Use GridSearchCV to find the best combination of hyperparameters\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8173374613003096\n",
      "Cross-Validation Scores: [0.80464217 0.8120155  0.79651163 0.81395349 0.78488372]\n",
      "Mean CV Accuracy: 0.802401301485913\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest classifier with the best hyperparameters\n",
    "best_rf_classifier = RandomForestClassifier(n_jobs=-1, **best_params)\n",
    "best_rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_rf)\n",
    "\n",
    "# Perform Cross-Validation with the best hyperparameters\n",
    "cv_scores_rf = cross_val_score(best_rf_classifier, X_train, y_train, cv=5, scoring='accuracy',n_jobs=-1)\n",
    "print(\"Cross-Validation Scores:\", cv_scores_rf)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Parameter Tuning with Cross-Validation\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      # Number of boosting stages to be used\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4],  # Step size shrinks the contribution of each tree\n",
    "    'max_depth': [5, 6, 7, 8]              # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2, tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model,param_grid,cv=5,scoring='accuracy',n_jobs=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.871517027863777\n",
      "Cross-Validation Scores: [0.86653772 0.89341085 0.87596899 0.86821705 0.8624031 ]\n",
      "Mean CV Accuracy: 0.8733075435203095\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boosting classifier with the best hyperparameters\n",
    "best_gb_classifier = xgb.XGBClassifier(\n",
    "    n_jobs=multiprocessing.cpu_count() // 2, tree_method=\"hist\", **best_params)\n",
    "best_gb_classifier.fit(X_train, y_train,verbose=3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_gb_classifier.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy_gb)\n",
    "\n",
    "# Perform Cross-Validation with the best hyperparameters\n",
    "cv_scores_gb = cross_val_score(best_gb_classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cv_scores_gb)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Mean CV Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.871517</td>\n",
       "      <td>0.873308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.839009</td>\n",
       "      <td>0.803554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>0.817337</td>\n",
       "      <td>0.802401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.716718</td>\n",
       "      <td>0.712510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.772446</td>\n",
       "      <td>0.611006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  Test Accuracy  Mean CV Accuracy\n",
       "4       Gradient Boosting       0.871517          0.873308\n",
       "2           Decision Tree       0.839009          0.803554\n",
       "3          Random Forrest       0.817337          0.802401\n",
       "0     Logistic Regression       0.716718          0.712510\n",
       "1  Support Vector Machine       0.772446          0.611006"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Algorithm', 'Test Accuracy', 'Mean CV Accuracy'])\n",
    "df.loc[len(df)] = ['Logistic Regression', accuracy_lr, np.mean(cv_scores_lr)]\n",
    "df.loc[len(df)] = ['Support Vector Machine', accuracy_svm, np.mean(cv_scores_svm)]\n",
    "df.loc[len(df)] = ['Decision Tree', accuracy_dt, np.mean(cv_scores_dt)]\n",
    "df.loc[len(df)] = ['Random Forrest', accuracy_rf, np.mean(cv_scores_rf)]\n",
    "df.loc[len(df)] = ['Gradient Boosting', accuracy_gb, np.mean(cv_scores_gb)]\n",
    "df.sort_values(by='Mean CV Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Gradient boosting was the best algorithm. We will use it to classify novel materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of novel Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = pd.read_csv('gap_prediction_random.csv')\n",
    "\n",
    "random_df[\"Space Group\"] = random_df[\"Space Group\"].astype('category')\n",
    "random_df['Space Group'] = random_df['Space Group'].map(mapping_dict)\n",
    "\n",
    "random_df.drop(['Material','Unnamed: 0'], axis='columns', inplace=True)\n",
    "X_random = random_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_gb_classifier.predict(X_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPhysics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
